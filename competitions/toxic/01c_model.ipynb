{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.layers import Dense, Dropout, GRU, Embedding \n",
    "from keras.layers import Input, Activation, concatenate, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils, get_custom_objects\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras_contrib.callbacks import SnapshotCallbackBuilder\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "SEQ_LENGTH = 200\n",
    "EMBED_SIZE = 100\n",
    "VOCAB = 173256\n",
    "USABLE = 100000\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(matrix):\n",
    "    rnn = {}\n",
    "    rnn['units'] = 50\n",
    "    rnn['return_sequences'] = True\n",
    "    rnn['recurrent_dropout'] = 0.2\n",
    "    rnn['dropout'] = 0.1\n",
    "    rnn['activation'] = 'tanh'\n",
    "    inputs = Input(shape=(SEQ_LENGTH,), name='sequence')\n",
    "    embed = Embedding(VOCAB,EMBED_SIZE, weights=[matrix], trainable=False)(inputs)\n",
    "    embed = SpatialDropout1D(0.2)(embed)\n",
    "    lstm_1 = Bidirectional(GRU(**rnn))(embed)\n",
    "    lstm_1 = BatchNormalization()(lstm_1)\n",
    "    rnn['return_sequences'] = False\n",
    "    lstm_2 = Bidirectional(GRU(**rnn))(embed)\n",
    "    lstm_2 = BatchNormalization()(lstm_2)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm_1)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm_1)\n",
    "    pool = concatenate([lstm_2, max_pool, avg_pool])\n",
    "    pool = BatchNormalization()(pool)\n",
    "    lstm = Dropout(0.2)(pool)\n",
    "    dense = Dense(256, activation='swish')(lstm)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(256, activation='swish')(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    predict = Dense(6, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=[inputs], output=predict)\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62953 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open('../data/data/fasttext/vector.vec')\n",
    "skip = True\n",
    "\n",
    "for line in f:\n",
    "    if not skip:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    else:\n",
    "        skip = False\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "matrix = np.stack(embeddings_index.values())\n",
    "mean, std = matrix.mean(), matrix.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataflow(train_text, valid_text):\n",
    "    train_text['comment_text'] = train_text['comment_text'].fillna('nan')\n",
    "    valid_text['comment_text'] = valid_text['comment_text'].fillna('nan')\n",
    "    train_text = list(train_text['comment_text'].values)\n",
    "    valid_text = list(valid_text['comment_text'].values)\n",
    "    tokenizer = text.Tokenizer(lower=True, char_level=False, num_words=USABLE)\n",
    "    tokenizer.fit_on_texts(train_text + valid_text)\n",
    "    word_index = tokenizer.word_index\n",
    "    intersect = 0\n",
    "    embedding_matrix = np.random.normal(mean, std, (len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            intersect += 1\n",
    "    train_token = tokenizer.texts_to_sequences(train_text)\n",
    "    valid_token = tokenizer.texts_to_sequences(valid_text)\n",
    "    train_seq = sequence.pad_sequences(train_token, maxlen=SEQ_LENGTH)\n",
    "    valid_seq = sequence.pad_sequences(valid_token, maxlen=SEQ_LENGTH)\n",
    "    return train_seq, valid_seq, embedding_matrix\n",
    "\n",
    "def callbacks(suffix):\n",
    "    stop = EarlyStopping('val_loss', patience=12, mode=\"min\")\n",
    "    snap = SnapshotCallbackBuilder(12,2,1e-3)\n",
    "    snap = snap.get_callbacks('model_{}'.format(suffix))\n",
    "    logger = CSVLogger('../data/data/source_1/model_4/logger_{}.log'.format(suffix))\n",
    "    return snap + [stop, logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def execute(mode):\n",
    "    mode += 1 \n",
    "    train_text = pd.read_csv('../data/data/source_1/train/train_data_{}.csv'.format(mode))\n",
    "    train_label = pd.read_csv('../data/data/source_1/train/train_labels_{}.csv'.format(mode))\n",
    "    valid_text = pd.read_csv('../data/data/source_1/train/test_data_{}.csv'.format(mode))\n",
    "    valid_label = pd.read_csv('../data/data/source_1/train/test_labels_{}.csv'.format(mode))\n",
    "    train_text, valid_text, embedding_matrix = dataflow(train_text, valid_text)\n",
    "    params = {}\n",
    "    params['x'] = train_text\n",
    "    params['y'] = np.array(train_label.iloc[:,1:])\n",
    "    params['validation_data'] = (valid_text, np.array(valid_label.iloc[:,1:]))\n",
    "    params['batch_size'] = 256\n",
    "    params['epochs'] = 30\n",
    "    params['verbose'] = 0\n",
    "    params['callbacks'] = callbacks(mode)\n",
    "    model = define_model(embedding_matrix)\n",
    "    model.fit(**params)\n",
    "    print('executed model:', mode)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed model: 3\n",
      "executed model: 1\n",
      "executed model: 2\n",
      "executed model: 4\n",
      "executed model: 6\n",
      "executed model: 5\n",
      "executed model: 7\n",
      "executed model: 8\n",
      "executed model: 9\n",
      "executed model: 10\n"
     ]
    }
   ],
   "source": [
    "pool = Pool(3)\n",
    "results = pool.map(execute, range(10))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
