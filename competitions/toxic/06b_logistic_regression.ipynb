{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(2, 6), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=1,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train = pd.read_csv('../data/download/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../data/download/test.csv').fillna(' ')\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "params ={}\n",
    "params['analyzer'] = 'word'\n",
    "params['ngram_range'] = (1,1)\n",
    "params['token_pattern'] = r'\\w{1,}'\n",
    "params['stop_words'] = 'english'\n",
    "params['strip_accents'] = 'unicode'\n",
    "params['sublinear_tf'] = 1\n",
    "params['max_features'] = 10000\n",
    "word_vectorizer = TfidfVectorizer(**params)\n",
    "word_vectorizer.fit(all_text)\n",
    "\n",
    "params ={}\n",
    "params['analyzer'] = 'char'\n",
    "params['ngram_range'] = (2,6)\n",
    "params['token_pattern'] = r'\\w{1,}'\n",
    "params['stop_words'] = 'english'\n",
    "params['strip_accents'] = 'unicode'\n",
    "params['sublinear_tf'] = 1\n",
    "params['max_features'] = 50000\n",
    "char_vectorizer = TfidfVectorizer(**params)\n",
    "char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_labels, train_text, valid_text, test_text):\n",
    "    train_word_doc = word_vectorizer.fit_transform(train_text['comment_text'].fillna('nan'))\n",
    "    valid_word_doc = word_vectorizer.transform(valid_text['comment_text'].fillna('nan'))\n",
    "    test_word_doc = word_vectorizer.transform(test_text['comment_text'].fillna('nan'))\n",
    "    train_char_doc = char_vectorizer.fit_transform(train_text['comment_text'].fillna('nan'))\n",
    "    valid_char_doc = char_vectorizer.transform(valid_text['comment_text'].fillna('nan'))\n",
    "    test_char_doc = char_vectorizer.transform(test_text['comment_text'].fillna('nan'))\n",
    "    train_features = hstack([train_char_doc, train_word_doc])\n",
    "    valid_features = hstack([valid_char_doc, valid_word_doc])\n",
    "    test_features = hstack([test_char_doc, test_word_doc])\n",
    "    valid_ids = valid_text[['id']].copy()\n",
    "    test_ids = test_text[['id']].copy()\n",
    "    valid_score = np.zeros([valid_text.shape[0],train_labels.shape[1]-1])\n",
    "    test_score = np.zeros([test_text.shape[0],train_labels.shape[1]-1])\n",
    "    for idx in range(train_labels.shape[1]-1):\n",
    "        model = LogisticRegression(C=0.1, solver='sag')\n",
    "        model.fit(train_features, train_labels.iloc[:,idx+1].values)\n",
    "        valid_score[:,idx] = model.predict_proba(valid_features)[:,1]\n",
    "        test_score[:,idx] = model.predict_proba(test_features)[:,1]\n",
    "    valid_score = pd.DataFrame(valid_score)\n",
    "    valid_score.columns = list(train_labels.columns)[1:]\n",
    "    valid_score = valid_ids.join(valid_score)\n",
    "    test_score = pd.DataFrame(test_score)\n",
    "    test_score.columns = list(train_labels.columns)[1:]\n",
    "    test_score = test_ids.join(test_score)\n",
    "    return valid_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(mode):\n",
    "    train_text = pd.read_csv('../data/data/source_6/train/train_data_{}.csv'.format(mode))\n",
    "    train_label = pd.read_csv('../data/data/source_6/train/train_labels_{}.csv'.format(mode))\n",
    "    valid_text = pd.read_csv('../data/data/source_6/train/test_data_{}.csv'.format(mode))\n",
    "    score_text = pd.read_csv('../data/data/source_6/score/score_data.csv'.format(mode))\n",
    "    valid_score, test_score = model(train_label, train_text, valid_text, score_text)\n",
    "    return valid_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 model executed.\n",
      "1 model executed.\n",
      "2 model executed.\n"
     ]
    }
   ],
   "source": [
    "valid_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for idx in range(9):\n",
    "    temp1, temp2 = execute(idx+1)\n",
    "    valid_scores.append(temp1)\n",
    "    test_scores.append(temp2)\n",
    "    print('{} model executed.'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: (143614, 7)\n",
      "Scoring: (153164, 7)\n"
     ]
    }
   ],
   "source": [
    "valid_data = reduce(lambda x,y : x.append(y), valid_scores)\n",
    "score_data = reduce(lambda x,y : x.append(y), test_scores)\n",
    "score_data = score_data.groupby('id').mean().reset_index()\n",
    "print('Cross Validation:', valid_data.shape)\n",
    "print('Scoring:', score_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.to_csv('../data/model/ftrl.csv', index=False)\n",
    "score_data.to_csv('../data/submit/ftrl.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../data/download/train.csv')\n",
    "labels = labels.drop('comment_text', axis=1)\n",
    "labels = valid_data[['id']].merge(labels, on='id')\n",
    "print('data:', valid_data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "evaluate = 0.\n",
    "\n",
    "for subset in models:\n",
    "    predict = valid_data[subset]\n",
    "    actual = labels[subset]\n",
    "    fpr, tpr, threshold = roc_curve(actual, predict)\n",
    "    metric = round(2*auc(fpr, tpr)-1, 4)\n",
    "    print('label:', subset, ':', metric)\n",
    "    evaluate += metric\n",
    "    \n",
    "print('overall:', round(evaluate/6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
