{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (143614, 7) (143614, 25) (153164, 25)\n"
     ]
    }
   ],
   "source": [
    "train_stack = pd.read_csv('../data/train_stack_scores.csv')\n",
    "train_labels = pd.read_csv('../data/download/train.csv').drop('comment_text', axis=1)\n",
    "train_labels = train_stack[['id']].merge(train_labels, on='id').reset_index(drop=True)\n",
    "test_stack = pd.read_csv('../data/test_stack_scores.csv')\n",
    "print('data:', train_labels.shape, train_stack.shape, test_stack.shape)\n",
    "train_ids = train_stack[['id']].copy()\n",
    "test_ids = test_stack[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['max_depth'] = 4\n",
    "params['metric'] = 'auc'\n",
    "params['n_estimators'] = 200\n",
    "params['num_leaves'] = 12\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['learning_rate'] = 0.075\n",
    "params['bagging_fraction'] = 0.8\n",
    "params['bagging_freq'] = 5\n",
    "params['reg_lambda'] = 0.2   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic : 0.9879587798\n",
      "severe_toxic : 0.99197915181\n",
      "obscene : 0.995425183592\n",
      "threat : 0.994613010272\n",
      "insult : 0.989955227643\n",
      "identity_hate : 0.99184535978\n",
      "overall: 0.991962785483\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(random_state=2017, n_splits=10)\n",
    "\n",
    "valid_scores = []\n",
    "valid_labels = []\n",
    "test_scores = []\n",
    "\n",
    "for train_idx, valid_idx in folds.split(train_ids):\n",
    "    train_idx = train_ids.iloc[train_idx]['id']\n",
    "    valid_idx = train_ids.iloc[valid_idx]['id']\n",
    "    X_train = train_stack[train_stack['id'].isin(train_idx.values)].drop('id',axis=1)\n",
    "    X_valid = train_stack[train_stack['id'].isin(valid_idx.values)].drop('id',axis=1)\n",
    "    X_score = test_stack.drop('id',axis=1).copy()\n",
    "    y_train = train_labels[train_labels['id'].isin(train_idx.values)].drop('id',axis=1)\n",
    "    y_valid = train_labels[train_labels['id'].isin(valid_idx.values)].drop('id',axis=1)\n",
    "    valid_score = np.zeros_like(y_valid)\n",
    "    valid_score = pd.DataFrame(valid_score)\n",
    "    test_score = np.zeros([X_score.shape[0],6])\n",
    "    test_score = pd.DataFrame(test_score)\n",
    "    valid_label = np.zeros_like(y_valid)\n",
    "    valid_label = pd.DataFrame(valid_label)\n",
    "    valid_score.columns = train_labels.columns[1:]\n",
    "    valid_label.columns = train_labels.columns[1:]\n",
    "    test_score.columns = train_labels.columns[1:]\n",
    "    for label in train_labels.columns[1:]:\n",
    "        _X_train = X_train[['rnn_' + label,'nbsvm_' + label,'logreg_' + label,'ftrl_' + label]]\n",
    "        _X_valid = X_valid[['rnn_' + label,'nbsvm_' + label,'logreg_' + label,'ftrl_' + label]]\n",
    "        _X_score = X_score[['rnn_' + label,'nbsvm_' + label,'logreg_' + label,'ftrl_' + label]]\n",
    "        _y_train = y_train[label]\n",
    "        _y_valid = y_valid[label]\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(_X_train, _y_train.values)\n",
    "        valid_score[label] = model.predict_proba(_X_valid)[:,1]\n",
    "        valid_score[label] = model.predict_proba(_X_valid)[:,1]\n",
    "        test_score[label] = model.predict_proba(_X_score)[:,1]\n",
    "        valid_label[label] = y_valid[label].values\n",
    "    valid_label['id'] = valid_idx\n",
    "    valid_score['id'] = valid_idx\n",
    "    test_score = test_ids.join(test_score)\n",
    "    valid_scores.append(valid_score)\n",
    "    test_scores.append(test_score)\n",
    "    valid_labels.append(valid_label)\n",
    "    \n",
    "valid_scores = reduce(lambda x,y : x.append(y), valid_scores)\n",
    "test_scores = reduce(lambda x,y : x.append(y), test_scores)\n",
    "valid_labels = reduce(lambda x,y : x.append(y), valid_labels)\n",
    "overall = 0.\n",
    "for label in train_labels.columns[1:]:\n",
    "    fpr,tpr,threshold = roc_curve(valid_labels[label], valid_scores[label])\n",
    "    overall += auc(fpr,tpr)\n",
    "    print(label,':',auc(fpr,tpr))\n",
    "print('overall:', overall/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores = test_scores.groupby('id').mean().reset_index()\n",
    "test_scores.to_csv('../data/test_lgb_stack.csv', index=False)\n",
    "test_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143614, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_scores = valid_scores[test_scores.columns]\n",
    "valid_scores.to_csv('../data/train_lgb_stack.csv', index=False)\n",
    "valid_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "stack = pd.read_csv('../data/train_lgb_stack.csv')\n",
    "blend = pd.read_csv('../data/train_stack_scores.csv')\n",
    "blend = blend[['id'] + [x for x in blend.columns if 'rnn' in x]]\n",
    "blend.columns = stack.columns\n",
    "score = stack.append(blend).groupby('id').mean().reset_index()\n",
    "labels = valid_labels.copy()\n",
    "labels = labels[test_scores.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic : 0.987773950322\n",
      "severe_toxic : 0.992358405397\n",
      "obscene : 0.995330043908\n",
      "threat : 0.994227122123\n",
      "insult : 0.990119283817\n",
      "identity_hate : 0.992142152589\n",
      "overall: 0.991991826359\n"
     ]
    }
   ],
   "source": [
    "overall = 0.\n",
    "for label in train_labels.columns[1:]:\n",
    "    fpr,tpr,threshold = roc_curve(labels[label], score[label])\n",
    "    overall += auc(fpr,tpr)\n",
    "    print(label,':',auc(fpr,tpr))\n",
    "print('overall:', overall/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_csv('../data/train_lgb_rnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack = pd.read_csv('../data/test_lgb_stack.csv')\n",
    "blend = pd.read_csv('../data/test_stack_scores.csv')\n",
    "blend = blend[['id'] + [x for x in blend.columns if 'rnn' in x]]\n",
    "blend.columns = stack.columns\n",
    "score = stack.append(blend).groupby('id').mean().reset_index()\n",
    "score.to_csv('../data/test_lgb_rnn.csv', index=False)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### masj blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_1 = pd.read_csv('../data/other/stack/masj_stacking.csv')\n",
    "blend_2 = pd.read_csv('../data/test_lgb_rnn.csv')\n",
    "blend = blend_1.append(blend_2).groupby('id').mean().reset_index()\n",
    "blend.to_csv('../data/test_lgb_rnn_masj.csv', index=False)\n",
    "blend.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post - processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   mark\n",
       "0  00001cee341fdb12  False\n",
       "1  0000247867823ef7  False\n",
       "2  00013b17ad220c46  False\n",
       "3  00017563c3f7919a  False\n",
       "4  00017695ad8997eb  False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreign = pd.read_csv('../data/download/test.csv')\n",
    "foreign['mark'] = foreign.comment_text.map(lambda x : min([c not in x.lower() for c in \"abcdefghijklmnopqrst\"])\n",
    "                                          and len(x) > 0)\n",
    "foreign = foreign[['id','mark']]\n",
    "foreign.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.read_csv('../data/test_lgb_rnn_masj.csv')\n",
    "score = foreign.merge(score, on='id')\n",
    "score.loc[score['mark'] == True,'toxic'] = 0.\n",
    "score.loc[score['mark'] == True,'severe_toxic'] = 0.\n",
    "score.loc[score['mark'] == True,'obscene'] = 0.\n",
    "score.loc[score['mark'] == True,'threat'] = 0.\n",
    "score.loc[score['mark'] == True,'insult'] = 0.\n",
    "score.loc[score['mark'] == True,'identity_hate'] = 0.\n",
    "score = score.drop('mark',axis=1)\n",
    "score.to_csv('../data/test_lgb_rnn_masj_pp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.read_csv('../data/test_lgb_rnn.csv')\n",
    "score = foreign.merge(score, on='id')\n",
    "score.loc[score['mark'] == True,'toxic'] = 0.\n",
    "score.loc[score['mark'] == True,'severe_toxic'] = 0.\n",
    "score.loc[score['mark'] == True,'obscene'] = 0.\n",
    "score.loc[score['mark'] == True,'threat'] = 0.\n",
    "score.loc[score['mark'] == True,'insult'] = 0.\n",
    "score.loc[score['mark'] == True,'identity_hate'] = 0.\n",
    "score = score.drop('mark',axis=1)\n",
    "score.to_csv('../data/test_lgb_rnn_pp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
