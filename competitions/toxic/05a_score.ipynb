{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.layers import Dense, Dropout, GRU, Embedding \n",
    "from keras.layers import Input, Activation, concatenate, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv1D, CuDNNGRU\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils, get_custom_objects\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras_contrib.callbacks import SnapshotCallbackBuilder\n",
    "from multiprocessing import Pool\n",
    "from keras.regularizers import L1L2\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "SEQ_LENGTH = 250\n",
    "EMBED_SIZE = 300\n",
    "VOCAB = 166930\n",
    "USABLE = 100000\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(matrix):\n",
    "    rnn = {}\n",
    "    rnn['units'] = 50\n",
    "    rnn['return_sequences'] = True\n",
    "    inputs = Input(shape=(SEQ_LENGTH,), name='sequence')\n",
    "    embed = Embedding(VOCAB,EMBED_SIZE, weights=[matrix], trainable=False)(inputs)\n",
    "    embed = SpatialDropout1D(0.4)(embed)\n",
    "    lstm = Bidirectional(CuDNNGRU(**rnn))(embed)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm)\n",
    "    lstm = Dropout(0.2)(lstm)\n",
    "    conv = Conv1D(64,4)(lstm)\n",
    "    conv = Conv1D(128,6)(conv)\n",
    "    conv_pool = GlobalMaxPooling1D()(conv)\n",
    "    pool = concatenate([max_pool, avg_pool,conv_pool])\n",
    "    pool = BatchNormalization()(pool)\n",
    "    pool = Dropout(0.3)(pool)\n",
    "    dense = Dense(256, activation='swish')(pool)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(256, activation='swish')(pool)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    predict = Dense(6, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=[inputs], output=predict)\n",
    "    optimizer = Adam(lr=1e-3, clipnorm=1.)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open('../data/download/glove.840B.300d.txt')\n",
    "skip = False\n",
    "\n",
    "for line in f:\n",
    "    if not skip:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    else:\n",
    "        skip = False\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "matrix = np.stack(embeddings_index.values())\n",
    "mean, std = matrix.mean(), matrix.std()\n",
    "del matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataflow(train_text, valid_text, score_text):\n",
    "    train_text['comment_text'] = train_text['comment_text'].fillna('nan')\n",
    "    valid_text['comment_text'] = valid_text['comment_text'].fillna('nan')\n",
    "    score_text['comment_text'] = score_text['comment_text'].fillna('nan')\n",
    "    train_text = list(train_text['comment_text'].values)\n",
    "    valid_text = list(valid_text['comment_text'].values)\n",
    "    score_text = list(score_text['comment_text'].values)\n",
    "    tokenizer = text.Tokenizer(lower=True, char_level=False, num_words=USABLE)\n",
    "    tokenizer.fit_on_texts(train_text + valid_text)\n",
    "    word_index = tokenizer.word_index\n",
    "    intersect = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            intersect += 1\n",
    "    score_token = tokenizer.texts_to_sequences(score_text)\n",
    "    score_seq = sequence.pad_sequences(score_token, maxlen=SEQ_LENGTH)\n",
    "    return score_seq, embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(mode):\n",
    "    train_text = pd.read_csv('../data/data/source_5/train/train_data_{}.csv'.format(mode))\n",
    "    valid_text = pd.read_csv('../data/data/source_5/train/test_data_{}.csv'.format(mode))\n",
    "    score_text = pd.read_csv('../data/data/source_5/score/score_data.csv')\n",
    "    score_data = score_text[['id']]\n",
    "    score_text, embedding_matrix = dataflow(train_text, valid_text, score_text)\n",
    "    model = define_model(embedding_matrix)\n",
    "    path = './weights/model_{}-Best.h5'.format(mode)\n",
    "    model.load_weights(path)\n",
    "    scores = model.predict(score_text, batch_size=256)\n",
    "    scores = pd.DataFrame(scores)\n",
    "    scores.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "    scores = score_data.join(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 model scored...\n",
      "2 model scored...\n",
      "3 model scored...\n",
      "4 model scored...\n",
      "5 model scored...\n",
      "6 model scored...\n",
      "7 model scored...\n",
      "8 model scored...\n",
      "9 model scored...\n"
     ]
    }
   ],
   "source": [
    "submit = pd.DataFrame([])\n",
    "\n",
    "for idx in range(9):\n",
    "    submit = submit.append(score_model(idx+1))\n",
    "    print('{} model scored...'.format(idx+1))\n",
    "    \n",
    "submit = submit.groupby('id').mean().reset_index()\n",
    "submit.to_csv('../data/submit/model_12.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(mode):\n",
    "    train_text = pd.read_csv('../data/data/source_5/train/train_data_{}.csv'.format(mode))\n",
    "    valid_text = pd.read_csv('../data/data/source_5/train/test_data_{}.csv'.format(mode))\n",
    "    score_text = pd.read_csv('../data/data/source_5/train/test_data_{}.csv'.format(mode))\n",
    "    labels = pd.read_csv('../data/data/source_5/train/test_labels_{}.csv'.format(mode))\n",
    "    score_data = score_text[['id']]\n",
    "    score_text, embedding_matrix = dataflow(train_text, valid_text, score_text)\n",
    "    model = define_model(embedding_matrix)\n",
    "    path = './weights/model_{}-Best.h5'.format(mode)\n",
    "    model.load_weights(path)\n",
    "    scores = model.predict(score_text, batch_size=256)\n",
    "    scores = pd.DataFrame(scores)\n",
    "    scores.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "    scores = score_data.join(scores)\n",
    "    return scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 model scored...\n",
      "2 model scored...\n",
      "3 model scored...\n",
      "4 model scored...\n",
      "5 model scored...\n",
      "6 model scored...\n",
      "7 model scored...\n",
      "8 model scored...\n",
      "9 model scored...\n"
     ]
    }
   ],
   "source": [
    "submit = pd.DataFrame([])\n",
    "labels = pd.DataFrame([])\n",
    "for idx in range(9):\n",
    "    submit_, labels_ = score_model(idx+1)\n",
    "    submit = submit.append(submit_)\n",
    "    labels = labels.append(labels_)\n",
    "    print('{} model scored...'.format(idx+1))\n",
    "    \n",
    "submit.to_csv('../data/model/model_12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: toxic : 0.9707\n",
      "label: severe_toxic : 0.9833\n",
      "label: obscene : 0.989\n",
      "label: threat : 0.9803\n",
      "label: insult : 0.978\n",
      "label: identity_hate : 0.9808\n",
      "overall: 0.9804\n"
     ]
    }
   ],
   "source": [
    "models = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "evaluate = 0.\n",
    "\n",
    "for subset in models:\n",
    "    predict = submit[subset]\n",
    "    actual = labels[subset]\n",
    "    fpr, tpr, threshold = roc_curve(actual, predict)\n",
    "    metric = round(2*auc(fpr, tpr)-1, 4)\n",
    "    print('label:', subset, ':', metric)\n",
    "    evaluate += metric\n",
    "    \n",
    "print('overall:', round(evaluate/6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
