{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.layers import Dense, Dropout, GRU, Embedding \n",
    "from keras.layers import Input, Activation, concatenate, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv1D, CuDNNGRU\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils, get_custom_objects\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras_contrib.callbacks import SnapshotCallbackBuilder\n",
    "from multiprocessing import Pool\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "SEQ_LENGTH = 250\n",
    "EMBED_SIZE = 300\n",
    "VOCAB = 166930\n",
    "USABLE = 100000\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(matrix):\n",
    "    rnn = {}\n",
    "    rnn['units'] = 50\n",
    "    rnn['return_sequences'] = True\n",
    "    inputs = Input(shape=(SEQ_LENGTH,), name='sequence')\n",
    "    embed = Embedding(VOCAB,EMBED_SIZE, weights=[matrix], trainable=False)(inputs)\n",
    "    embed = SpatialDropout1D(0.4)(embed)\n",
    "    lstm = Bidirectional(CuDNNGRU(**rnn))(embed)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm)\n",
    "    lstm = Dropout(0.2)(lstm)\n",
    "    conv = Conv1D(64,4)(lstm)\n",
    "    conv = Conv1D(128,6)(conv)\n",
    "    conv_pool = GlobalMaxPooling1D()(conv)\n",
    "    pool = concatenate([max_pool, avg_pool,conv_pool])\n",
    "    pool = BatchNormalization()(pool)\n",
    "    pool = Dropout(0.3)(pool)\n",
    "    dense = Dense(256, activation='swish')(pool)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(256, activation='swish')(pool)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    predict = Dense(6, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=[inputs], output=predict)\n",
    "    optimizer = Adam(lr=1e-3, clipnorm=1.)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open('../data/download/glove.840B.300d.txt')\n",
    "skip = False\n",
    "\n",
    "for line in f:\n",
    "    if not skip:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    else:\n",
    "        skip = False\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "matrix = np.stack(embeddings_index.values())\n",
    "mean, std = matrix.mean(), matrix.std()\n",
    "del matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataflow(train_text, valid_text):\n",
    "    train_text['comment_text'] = train_text['comment_text'].fillna('nan')\n",
    "    valid_text['comment_text'] = valid_text['comment_text'].fillna('nan')\n",
    "    train_text = list(train_text['comment_text'].values)\n",
    "    valid_text = list(valid_text['comment_text'].values)\n",
    "    tokenizer = text.Tokenizer(lower=True, char_level=False, num_words=USABLE)\n",
    "    tokenizer.fit_on_texts(train_text + valid_text)\n",
    "    word_index = tokenizer.word_index\n",
    "    intersect = 0\n",
    "    embedding_matrix = np.random.normal(mean, std, (len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            intersect += 1\n",
    "    train_token = tokenizer.texts_to_sequences(train_text)\n",
    "    valid_token = tokenizer.texts_to_sequences(valid_text)\n",
    "    train_seq = sequence.pad_sequences(train_token, maxlen=SEQ_LENGTH)\n",
    "    valid_seq = sequence.pad_sequences(valid_token, maxlen=SEQ_LENGTH)\n",
    "    return train_seq, valid_seq, embedding_matrix\n",
    "\n",
    "def callbacks(suffix):\n",
    "    stop = EarlyStopping('val_loss', patience=18, mode=\"min\")\n",
    "    snap = SnapshotCallbackBuilder(30,3,1e-3)\n",
    "    snap = snap.get_callbacks('model_{}'.format(suffix))\n",
    "    logger = CSVLogger('../data/data/source_5/model_1/logger_{}.log'.format(suffix))\n",
    "    return snap + [stop, logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def execute(mode):\n",
    "    mode += 1 \n",
    "    train_text = pd.read_csv('../data/data/source_5/train/train_data_{}.csv'.format(mode))\n",
    "    train_label = pd.read_csv('../data/data/source_5/train/train_labels_{}.csv'.format(mode))\n",
    "    valid_text = pd.read_csv('../data/data/source_5/train/test_data_{}.csv'.format(mode))\n",
    "    valid_label = pd.read_csv('../data/data/source_5/train/test_labels_{}.csv'.format(mode))\n",
    "    train_text, valid_text, embedding_matrix = dataflow(train_text, valid_text)\n",
    "    params = {}\n",
    "    params['x'] = train_text\n",
    "    params['y'] = np.array(train_label.iloc[:,1:])\n",
    "    params['validation_data'] = (valid_text, np.array(valid_label.iloc[:,1:]))\n",
    "    params['batch_size'] = 256\n",
    "    params['epochs'] = 30\n",
    "    params['verbose'] = 0\n",
    "    params['callbacks'] = callbacks(mode)\n",
    "    model = define_model(embedding_matrix)\n",
    "    model.fit(**params)\n",
    "    print('executed model:', mode)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed model: 1\n",
      "executed model: 2\n",
      "executed model: 3\n",
      "executed model: 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    execute(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
