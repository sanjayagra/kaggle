{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import re\n",
    "import string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(document, y_i, y):\n",
    "    p = document[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "def compute(document, labels):\n",
    "    y = labels.values\n",
    "    r = np.log(probability(document,1,y) / probability(document,0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = document.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "def model(train_labels, train_text, valid_text, test_text):\n",
    "    n = train_text.shape[0]\n",
    "    params ={}\n",
    "    params['ngram_range'] = (1,2)\n",
    "    params['tokenizer'] = tokenize\n",
    "    params['min_df'] = 3\n",
    "    params['max_df'] = 0.9\n",
    "    params['strip_accents'] = 'unicode'\n",
    "    params['use_idf'] = 1\n",
    "    params['smooth_idf'] = 1\n",
    "    params['sublinear_tf'] = 1\n",
    "    vectorize = TfidfVectorizer(**params)\n",
    "    train_doc = vectorize.fit_transform(train_text['comment_text'].fillna('nan'))\n",
    "    valid_doc = vectorize.transform(valid_text['comment_text'].fillna('nan'))\n",
    "    test_doc = vectorize.transform(test_text['comment_text'].fillna('nan'))\n",
    "    valid_ids = valid_text[['id']].copy()\n",
    "    test_ids = test_text[['id']].copy()\n",
    "    valid_score = np.zeros([valid_text.shape[0],train_labels.shape[1]-1])\n",
    "    test_score = np.zeros([test_text.shape[0],train_labels.shape[1]-1])\n",
    "    for idx in range(train_labels.shape[1]-1):\n",
    "        model,result = compute(train_doc, train_labels.iloc[:,idx+1])\n",
    "        valid_score[:,idx] = model.predict_proba(valid_doc.multiply(result))[:,1]\n",
    "        test_score[:,idx] = model.predict_proba(test_doc.multiply(result))[:,1]\n",
    "    valid_score = pd.DataFrame(valid_score)\n",
    "    valid_score.columns = list(train_labels.columns)[1:]\n",
    "    valid_score = valid_ids.join(valid_score)\n",
    "    test_score = pd.DataFrame(test_score)\n",
    "    test_score.columns = list(train_labels.columns)[1:]\n",
    "    test_score = test_ids.join(test_score)\n",
    "    return valid_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(mode):\n",
    "    train_text = pd.read_csv('../data/data/source_6/train/train_data_{}.csv'.format(mode))\n",
    "    train_label = pd.read_csv('../data/data/source_6/train/train_labels_{}.csv'.format(mode))\n",
    "    valid_text = pd.read_csv('../data/data/source_6/train/test_data_{}.csv'.format(mode))\n",
    "    score_text = pd.read_csv('../data/data/source_6/score/score_data.csv'.format(mode))\n",
    "    valid_score, test_score = model(train_label, train_text, valid_text, score_text)\n",
    "    return valid_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 model executed.\n",
      "1 model executed.\n",
      "2 model executed.\n",
      "3 model executed.\n",
      "4 model executed.\n",
      "5 model executed.\n",
      "6 model executed.\n",
      "7 model executed.\n",
      "8 model executed.\n"
     ]
    }
   ],
   "source": [
    "valid_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for idx in range(9):\n",
    "    temp1, temp2 = execute(idx+1)\n",
    "    valid_scores.append(temp1)\n",
    "    test_scores.append(temp2)\n",
    "    print('{} model executed.'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: (143614, 7)\n",
      "Scoring: (153164, 7)\n"
     ]
    }
   ],
   "source": [
    "valid_data = reduce(lambda x,y : x.append(y), valid_scores)\n",
    "score_data = reduce(lambda x,y : x.append(y), test_scores)\n",
    "score_data = score_data.groupby('id').mean().reset_index()\n",
    "print('Cross Validation:', valid_data.shape)\n",
    "print('Scoring:', score_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.to_csv('../data/model/nbsvm.csv', index=False)\n",
    "score_data.to_csv('../data/submit/nbsvm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (143614, 7) (143614, 7)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv('../data/download/train.csv')\n",
    "labels = labels.drop('comment_text', axis=1)\n",
    "labels = valid_data[['id']].merge(labels, on='id')\n",
    "print('data:', valid_data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: toxic : 0.9601\n",
      "label: severe_toxic : 0.9705\n",
      "label: obscene : 0.9822\n",
      "label: threat : 0.97\n",
      "label: insult : 0.9661\n",
      "label: identity_hate : 0.9595\n",
      "overall: 0.9681\n"
     ]
    }
   ],
   "source": [
    "models = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "evaluate = 0.\n",
    "\n",
    "for subset in models:\n",
    "    predict = valid_data[subset]\n",
    "    actual = labels[subset]\n",
    "    fpr, tpr, threshold = roc_curve(actual, predict)\n",
    "    metric = round(2*auc(fpr, tpr)-1, 4)\n",
    "    print('label:', subset, ':', metric)\n",
    "    evaluate += metric\n",
    "    \n",
    "print('overall:', round(evaluate/6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
