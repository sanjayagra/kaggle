{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None\n",
    "stopwords = set(stopwords.words('english'))\n",
    "train_data = pd.read_csv('../../data/spooky-author/download/train.csv')\n",
    "test_data = pd.read_csv('../../data/spooky-author/download/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_string(x):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    porter = PorterStemmer()\n",
    "    x = x.lower().translate(table).split()\n",
    "    y = [porter.stem(y) for y in x]\n",
    "    return ' '.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer + SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (19579, 20)\n",
      "test: (8392, 20)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3), min_df=5)\n",
    "svd = TruncatedSVD(n_components=20, algorithm='arpack', random_state=123)\n",
    "train_text = list(map(clean_string, list(train_data.text.values)))\n",
    "test_text = list(map(clean_string, list(test_data.text.values)))\n",
    "full_text = train_text + test_text\n",
    "full_text = vectorizer.fit_transform(train_text + test_text)\n",
    "svd.fit(full_text)\n",
    "train_word  = svd.transform(vectorizer.transform(train_text))\n",
    "test_word  = svd.transform(vectorizer.transform(test_text))\n",
    "print('train:', train_word.shape)\n",
    "print('test:', test_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (19579, 20)\n",
      "test: (8392, 20)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3), min_df=5, analyzer='char')\n",
    "svd = TruncatedSVD(n_components=20, algorithm='arpack', random_state=123)\n",
    "train_text = list(map(clean_string, list(train_data.text.values)))\n",
    "test_text = list(map(clean_string, list(test_data.text.values)))\n",
    "full_text = train_text + test_text\n",
    "full_text = vectorizer.fit_transform(train_text + test_text)\n",
    "svd.fit(full_text.asfptype())\n",
    "train_char  = svd.transform(vectorizer.transform(train_text))\n",
    "test_char = svd.transform(vectorizer.transform(test_text))\n",
    "print('train:', train_char.shape)\n",
    "print('test:', test_char.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (19579, 40)\n",
      "test: (8392, 40)\n"
     ]
    }
   ],
   "source": [
    "train_feats = np.hstack([train_word, train_char])\n",
    "test_feats = np.hstack([test_word, test_char])\n",
    "scaler = MinMaxScaler()\n",
    "train_feats = scaler.fit_transform(train_feats)\n",
    "test_feats = scaler.transform(test_feats)\n",
    "print('train:', train_feats.shape)\n",
    "print('test:', test_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, random_state=2017, shuffle=True)\n",
    "mapper = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_data['author'] = train_data['author'].map(lambda x : mapper[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_score = pd.DataFrame([])\n",
    "test_score = pd.DataFrame([])\n",
    "test_score['id'] = test_data['id']\n",
    "\n",
    "for fold, idx in enumerate(folds.split(train_feats)):\n",
    "    train_id = idx[0]\n",
    "    test_id = idx[1]\n",
    "    valid_temp = pd.DataFrame([])\n",
    "    valid_temp['id'] = train_data.iloc[test_id,:]['id']\n",
    "    valid_temp['author'] = train_data.iloc[test_id,:]['author']\n",
    "    model = MultinomialNB()\n",
    "    X_train = train_feats[train_id,:]\n",
    "    y_train = train_data.iloc[train_id,:]['author']\n",
    "    X_valid = train_feats[test_id,:]\n",
    "    model.fit(X_train, y_train)\n",
    "    valid_temp['scores_0'] = model.predict_proba(X_valid)[:,0]\n",
    "    valid_temp['scores_1'] = model.predict_proba(X_valid)[:,1]\n",
    "    valid_temp['scores_2'] = model.predict_proba(X_valid)[:,2]\n",
    "    test_score['scores_0_' + str(fold)] = model.predict_proba(test_feats)[:,0]\n",
    "    test_score['scores_1_' + str(fold)] = model.predict_proba(test_feats)[:,1]\n",
    "    test_score['scores_2_' + str(fold)] = model.predict_proba(test_feats)[:,2]\n",
    "    valid_score = valid_score.append(valid_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score['scores_0']  = test_score['scores_0_0']\n",
    "test_score['scores_0'] += test_score['scores_0_1']\n",
    "test_score['scores_0'] += test_score['scores_0_2']\n",
    "test_score['scores_0'] += test_score['scores_0_3']\n",
    "test_score['scores_0'] += test_score['scores_0_4']\n",
    "test_score['scores_0'] = test_score['scores_0'] / 5\n",
    "\n",
    "test_score['scores_1']  = test_score['scores_1_0']\n",
    "test_score['scores_1'] += test_score['scores_1_1']\n",
    "test_score['scores_1'] += test_score['scores_1_2']\n",
    "test_score['scores_1'] += test_score['scores_1_3']\n",
    "test_score['scores_1'] += test_score['scores_1_4']\n",
    "test_score['scores_1'] = test_score['scores_1'] / 5\n",
    "\n",
    "test_score['scores_2']  = test_score['scores_2_0']\n",
    "test_score['scores_2'] += test_score['scores_2_1']\n",
    "test_score['scores_2'] += test_score['scores_2_2']\n",
    "test_score['scores_2'] += test_score['scores_2_3']\n",
    "test_score['scores_2'] += test_score['scores_2_4']\n",
    "test_score['scores_2'] = test_score['scores_2'] / 5\n",
    "\n",
    "test_score = test_score[['id','scores_0', 'scores_1','scores_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (19579, 4)\n",
      "test: (8392, 4)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "valid_score.iloc[:,2:] = scaler.fit_transform(valid_score.iloc[:,2:])\n",
    "valid_score.drop('author', axis=1, inplace=True)\n",
    "test_score.iloc[:,1:] = scaler.transform(test_score.iloc[:,1:])\n",
    "print('train:', valid_score.shape)\n",
    "print('test:', test_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_score.to_csv('../../data/spooky-author/data/train_nb_feats.csv')\n",
    "test_score.to_csv('../../data/spooky-author/data/test_nb_feats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
